{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e98fa156",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from IPython.display import clear_output\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e65875da",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('continuous_dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6a6ee7e8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>datetime</th>\n",
       "      <th>nat_demand</th>\n",
       "      <th>T2M_toc</th>\n",
       "      <th>QV2M_toc</th>\n",
       "      <th>TQL_toc</th>\n",
       "      <th>W2M_toc</th>\n",
       "      <th>T2M_san</th>\n",
       "      <th>QV2M_san</th>\n",
       "      <th>TQL_san</th>\n",
       "      <th>W2M_san</th>\n",
       "      <th>T2M_dav</th>\n",
       "      <th>QV2M_dav</th>\n",
       "      <th>TQL_dav</th>\n",
       "      <th>W2M_dav</th>\n",
       "      <th>Holiday_ID</th>\n",
       "      <th>holiday</th>\n",
       "      <th>school</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2015-01-03 01:00:00</td>\n",
       "      <td>970.3450</td>\n",
       "      <td>25.865259</td>\n",
       "      <td>0.018576</td>\n",
       "      <td>0.016174</td>\n",
       "      <td>21.850546</td>\n",
       "      <td>23.482446</td>\n",
       "      <td>0.017272</td>\n",
       "      <td>0.001855</td>\n",
       "      <td>10.328949</td>\n",
       "      <td>22.662134</td>\n",
       "      <td>0.016562</td>\n",
       "      <td>0.096100</td>\n",
       "      <td>5.364148</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2015-01-03 02:00:00</td>\n",
       "      <td>912.1755</td>\n",
       "      <td>25.899255</td>\n",
       "      <td>0.018653</td>\n",
       "      <td>0.016418</td>\n",
       "      <td>22.166944</td>\n",
       "      <td>23.399255</td>\n",
       "      <td>0.017265</td>\n",
       "      <td>0.001327</td>\n",
       "      <td>10.681517</td>\n",
       "      <td>22.578943</td>\n",
       "      <td>0.016509</td>\n",
       "      <td>0.087646</td>\n",
       "      <td>5.572471</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2015-01-03 03:00:00</td>\n",
       "      <td>900.2688</td>\n",
       "      <td>25.937280</td>\n",
       "      <td>0.018768</td>\n",
       "      <td>0.015480</td>\n",
       "      <td>22.454911</td>\n",
       "      <td>23.343530</td>\n",
       "      <td>0.017211</td>\n",
       "      <td>0.001428</td>\n",
       "      <td>10.874924</td>\n",
       "      <td>22.531030</td>\n",
       "      <td>0.016479</td>\n",
       "      <td>0.078735</td>\n",
       "      <td>5.871184</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2015-01-03 04:00:00</td>\n",
       "      <td>889.9538</td>\n",
       "      <td>25.957544</td>\n",
       "      <td>0.018890</td>\n",
       "      <td>0.016273</td>\n",
       "      <td>22.110481</td>\n",
       "      <td>23.238794</td>\n",
       "      <td>0.017128</td>\n",
       "      <td>0.002599</td>\n",
       "      <td>10.518620</td>\n",
       "      <td>22.512231</td>\n",
       "      <td>0.016487</td>\n",
       "      <td>0.068390</td>\n",
       "      <td>5.883621</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2015-01-03 05:00:00</td>\n",
       "      <td>893.6865</td>\n",
       "      <td>25.973840</td>\n",
       "      <td>0.018981</td>\n",
       "      <td>0.017281</td>\n",
       "      <td>21.186089</td>\n",
       "      <td>23.075403</td>\n",
       "      <td>0.017059</td>\n",
       "      <td>0.001729</td>\n",
       "      <td>9.733589</td>\n",
       "      <td>22.481653</td>\n",
       "      <td>0.016456</td>\n",
       "      <td>0.064362</td>\n",
       "      <td>5.611724</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              datetime  nat_demand    T2M_toc  QV2M_toc   TQL_toc    W2M_toc  \\\n",
       "0  2015-01-03 01:00:00    970.3450  25.865259  0.018576  0.016174  21.850546   \n",
       "1  2015-01-03 02:00:00    912.1755  25.899255  0.018653  0.016418  22.166944   \n",
       "2  2015-01-03 03:00:00    900.2688  25.937280  0.018768  0.015480  22.454911   \n",
       "3  2015-01-03 04:00:00    889.9538  25.957544  0.018890  0.016273  22.110481   \n",
       "4  2015-01-03 05:00:00    893.6865  25.973840  0.018981  0.017281  21.186089   \n",
       "\n",
       "     T2M_san  QV2M_san   TQL_san    W2M_san    T2M_dav  QV2M_dav   TQL_dav  \\\n",
       "0  23.482446  0.017272  0.001855  10.328949  22.662134  0.016562  0.096100   \n",
       "1  23.399255  0.017265  0.001327  10.681517  22.578943  0.016509  0.087646   \n",
       "2  23.343530  0.017211  0.001428  10.874924  22.531030  0.016479  0.078735   \n",
       "3  23.238794  0.017128  0.002599  10.518620  22.512231  0.016487  0.068390   \n",
       "4  23.075403  0.017059  0.001729   9.733589  22.481653  0.016456  0.064362   \n",
       "\n",
       "    W2M_dav  Holiday_ID  holiday  school  \n",
       "0  5.364148           0        0       0  \n",
       "1  5.572471           0        0       0  \n",
       "2  5.871184           0        0       0  \n",
       "3  5.883621           0        0       0  \n",
       "4  5.611724           0        0       0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "4b5b49f8",
   "metadata": {},
   "source": [
    "datetime: Date and time information.\n",
    "nat_demand: Demand for electricity.\n",
    "T2M_toc: Temperature at a certain location (toc).\n",
    "QV2M_toc: Specific humidity at a certain location (toc).\n",
    "TQL_toc: Total column liquid at a certain location (toc).\n",
    "W2M_toc: Wind speed at a certain location (toc).\n",
    "T2M_san: Temperature at another location (san).\n",
    "QV2M_san: Specific humidity at another location (san).\n",
    "TQL_san: Total column liquid at another location (san).\n",
    "W2M_san: Wind speed at another location (san).\n",
    "T2M_dav: Temperature at yet another location (dav).\n",
    "QV2M_dav: Specific humidity at yet another location (dav).\n",
    "TQL_dav: Total column liquid at yet another location (dav).\n",
    "W2M_dav: Wind speed at yet another location (dav).\n",
    "Holiday_ID: ID indicating whether it's a holiday.\n",
    "holiday: Information about holidays.\n",
    "school: Information about school schedules."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf511f3d",
   "metadata": {},
   "source": [
    "# Preprocess the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fdd16b9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing.sequence import TimeseriesGenerator\n",
    "\n",
    "class TabularLSTMDataPreprocessor:\n",
    "    def __init__(self, dataframe, target_column, time_column, categorical_columns=None,\n",
    "                 scaler='minmax', sequence_length=24, batch_size=32, random_state=None):\n",
    "        self.dataframe = dataframe\n",
    "        self.target_column = target_column\n",
    "        self.time_column = time_column\n",
    "        self.categorical_columns = categorical_columns if categorical_columns else []\n",
    "        self.scaler = self.get_scaler(scaler)\n",
    "        self.sequence_length = sequence_length\n",
    "        self.batch_size = batch_size\n",
    "        self.random_state = random_state\n",
    "\n",
    "    def get_scaler(self, scaler_type):\n",
    "        if scaler_type == 'minmax':\n",
    "            return MinMaxScaler()\n",
    "        elif scaler_type == 'standard':\n",
    "            return StandardScaler()\n",
    "        else:\n",
    "            raise ValueError(\"Invalid scaler type. Use 'minmax' or 'standard'.\")\n",
    "\n",
    "    def preprocess(self):\n",
    "       \n",
    "        windows = [12, 24, 128]\n",
    "        for column in self.dataframe.columns:\n",
    "            if column != self.time_column and column not in self.categorical_columns:\n",
    "                for window in windows:\n",
    "                    self.dataframe[f\"{column}_lag_{window}\"] = self.dataframe[column].shift(window)\n",
    "                   \n",
    "        # Drop rows with missing values\n",
    "        self.dataframe.dropna(inplace=True)\n",
    "\n",
    "        # Sort by time\n",
    "        self.dataframe.sort_values(by=[self.time_column], inplace=True)\n",
    "        \n",
    "        # Scale numerical features\n",
    "        numerical_columns = [col for col in self.dataframe.columns if col not in [self.target_column, self.time_column, self.categorical_columns]]\n",
    "        self.dataframe[numerical_columns] = self.scaler.fit_transform(self.dataframe[numerical_columns])\n",
    "\n",
    "        # Apply one-hot encoding to categorical columns (if any)\n",
    "        if self.categorical_columns:\n",
    "            self.dataframe = pd.get_dummies(self.dataframe, columns=self.categorical_columns, drop_first=True)\n",
    "        \n",
    "        train_df = self.dataframe[self.dataframe[self.time_column] < '2019-01-01']\n",
    "        test_df = self.dataframe[self.dataframe[self.time_column] >= '2019-01-01']\n",
    "        # Split data into train and test sets\n",
    "        X_train = train_df.drop(columns=[self.target_column, self.time_column]).values.astype(np.float32)\n",
    "        y_train = train_df[self.target_column].values.astype(np.float32)\n",
    "        #X = X.astype(np.float32)\n",
    "        #y = y.astype(np.float32)\n",
    "#        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=self.test_size, random_state=self.random_state, shuffle=False)\n",
    "        X_test = test_df.drop(columns=[self.target_column, self.time_column]).values.astype(np.float32)\n",
    "        y_test = test_df[self.target_column].values.astype(np.float32)\n",
    "        \n",
    "        # Create time series generators for training and testing\n",
    "        train_data_gen = TimeseriesGenerator(X_train, y_train,\n",
    "                                             length=self.sequence_length, batch_size=self.batch_size)\n",
    "        test_data_gen = TimeseriesGenerator(X_test, y_test,\n",
    "                                            length=self.sequence_length, batch_size=self.batch_size)\n",
    "    \n",
    "\n",
    "        return train_data_gen, test_data_gen \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a29dac6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the TabularLSTMDataPreprocessor\n",
    "data_preprocessor = TabularLSTMDataPreprocessor(df, target_column='nat_demand', time_column='datetime',\n",
    "                                                categorical_columns=['holiday', 'school', 'Holiday_ID'],\n",
    "                                                scaler='standard', sequence_length=24, batch_size=64)\n",
    "\n",
    "# Preprocess the data and obtain data generators\n",
    "\n",
    "train_data_gen, test_data_gen = data_preprocessor.preprocess()\n",
    "clear_output()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2274c4b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Flatten\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "# Define custom RMSE loss function\n",
    "def root_mean_squared_error(y_true, y_pred):\n",
    "    return K.sqrt(K.mean(K.square(y_pred - y_true)))\n",
    "\n",
    "def accuracy(y_true, y_pred):\n",
    "    return accuracy_score((y_true, y_pred))\n",
    "\n",
    "\n",
    "\n",
    "class TabularLSTMModel:\n",
    "    def __init__(self, input_shape, lstm_units=[64, 32], output_units=1):\n",
    "        self.input_shape = input_shape\n",
    "        self.lstm_units = lstm_units\n",
    "        self.output_units = output_units\n",
    "        self.model = self.build_model()\n",
    "\n",
    "    def build_model(self):\n",
    "        model = Sequential()\n",
    "        for units in self.lstm_units:\n",
    "            model.add(LSTM(units, return_sequences=True, input_shape=self.input_shape, activation='relu'))\n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(self.output_units))\n",
    "        return model\n",
    "\n",
    "    def compile(self, learning_rate=0.001):\n",
    "        optimizer = Adam(learning_rate=learning_rate)\n",
    "        self.model.compile(loss=root_mean_squared_error, optimizer=optimizer)\n",
    "\n",
    "    def fit(self, train_data_gen, epochs=10):\n",
    "        self.model.fit(train_data_gen, epochs=epochs)\n",
    "\n",
    "    def evaluate(self, test_data_gen):\n",
    "        return self.model.evaluate(test_data_gen)\n",
    "    \n",
    "    def predict(self, data_gen):\n",
    "        return self.model.predict(data_gen)\n",
    "    \n",
    "    def summary(self):\n",
    "        return self.model.summary()\n",
    "    \n",
    "    def save(self, filepath):\n",
    "        self.model.save(filepath)\n",
    "    \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "99061007",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_8 (LSTM)               (None, 24, 64)            35840     \n",
      "                                                                 \n",
      " lstm_9 (LSTM)               (None, 24, 32)            12416     \n",
      "                                                                 \n",
      " lstm_10 (LSTM)              (None, 24, 16)            3136      \n",
      "                                                                 \n",
      " lstm_11 (LSTM)              (None, 24, 8)             800       \n",
      "                                                                 \n",
      " flatten_2 (Flatten)         (None, 192)               0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 1)                 193       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 52,385\n",
      "Trainable params: 52,385\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-19 16:19:52.984015: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32\n",
      "\t [[{{node Placeholder/_0}}]]\n",
      "2024-04-19 16:19:53.015951: W tensorflow/tsl/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "545/545 [==============================] - 12s 20ms/step - loss: 950.1782\n",
      "Epoch 2/10\n",
      "545/545 [==============================] - 11s 20ms/step - loss: 402.8915\n",
      "Epoch 3/10\n",
      "545/545 [==============================] - 11s 21ms/step - loss: 278.1380\n",
      "Epoch 4/10\n",
      "545/545 [==============================] - 11s 21ms/step - loss: 219.6943\n",
      "Epoch 5/10\n",
      "545/545 [==============================] - 11s 21ms/step - loss: 228.5653\n",
      "Epoch 6/10\n",
      "545/545 [==============================] - 11s 21ms/step - loss: 682.5992\n",
      "Epoch 7/10\n",
      "545/545 [==============================] - 11s 21ms/step - loss: 640.8253\n",
      "Epoch 8/10\n",
      "545/545 [==============================] - 11s 21ms/step - loss: 270.9388\n",
      "Epoch 9/10\n",
      "545/545 [==============================] - 11s 21ms/step - loss: 949.7408\n",
      "Epoch 10/10\n",
      "545/545 [==============================] - 11s 20ms/step - loss: 615.5298\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-19 16:21:46.792607: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32\n",
      "\t [[{{node Placeholder/_0}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "204/204 [==============================] - 1s 5ms/step - loss: 482.0829\n",
      "Test Loss (RMSE): 482.0828552246094\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.models import save_model\n",
    "\n",
    "# Instantiate the TabularLSTMModel with two LSTM layers\n",
    "input_shape = (24, 75)\n",
    "lstm_units = [64, 32, 16, 8]  # Define the units for each LSTM layer\n",
    "lstm_model = TabularLSTMModel(input_shape, lstm_units)\n",
    "num_epochs = 10\n",
    "lstm_model.summary()\n",
    "\n",
    "\n",
    "# Compile the model\n",
    "lstm_model.compile(learning_rate=0.001)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# After saving the model\n",
    "lstm_model.model.save('lstm_model.h5')\n",
    "\n",
    "# Import the saved model, providing the custom loss function to custom_objects\n",
    "loaded_model = load_model('lstm_model.h5', custom_objects={'root_mean_squared_error': root_mean_squared_error})\n",
    "\n",
    "\n",
    "\n",
    "# Train and save the model\n",
    "lstm_model.fit(train_data_gen, epochs=num_epochs)\n",
    "lstm_model.save('trained_lstm_model.h5')\n",
    "\n",
    "# Evaluate the model on the test data\n",
    "loss = lstm_model.evaluate(test_data_gen)\n",
    "\n",
    "print(f'Test Loss (RMSE): {loss}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9726f458",
   "metadata": {},
   "outputs": [],
   "source": [
    "#***************************** RESULT *********************************\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, explained_variance_score, max_error, mean_poisson_deviance, mean_gamma_deviance, mean_tweedie_deviance, mean_absolute_percentage_error, accuracy_score\n",
    "\n",
    "class Result:\n",
    "    def __init__(self, model, test_data_gen):\n",
    "        self.model = model\n",
    "        self.test_data_gen = test_data_gen\n",
    "        self.y_true = None\n",
    "        self.y_pred = None\n",
    "\n",
    "    def evaluate(self):\n",
    "        y_true = []\n",
    "        y_pred = []\n",
    "\n",
    "        for i in range(len(self.test_data_gen)):\n",
    "            x_batch, y_batch = self.test_data_gen[i]\n",
    "            y_true_batch = y_batch\n",
    "            y_pred_batch = self.model.predict(x_batch)\n",
    "\n",
    "            # Append values to the lists within the loop\n",
    "            y_true.extend(y_true_batch)\n",
    "            y_pred.extend(y_pred_batch)\n",
    "\n",
    "        self.y_true = np.array(y_true).flatten()\n",
    "        self.y_pred = np.array(y_pred).flatten()\n",
    "\n",
    "        mae = mean_absolute_error(self.y_true, self.y_pred)\n",
    "        mse = mean_squared_error(self.y_true, self.y_pred)\n",
    "        rmse = np.sqrt(mse)\n",
    "        \n",
    "\n",
    "        # Calculate MAPE (Mean Absolute Percentage Error)\n",
    "        mape = mean_absolute_percentage_error(self.y_true, self.y_pred)\n",
    "        \n",
    "        r2 = r2_score(self.y_true, self.y_pred)\n",
    "        explained_variance = explained_variance_score(self.y_true, self.y_pred)\n",
    "        max_err = max_error(self.y_true, self.y_pred)\n",
    "        poisson_deviance = mean_poisson_deviance(self.y_true, self.y_pred)\n",
    "        gamma_deviance = mean_gamma_deviance(self.y_true, self.y_pred)\n",
    "        tweedie_deviance = mean_tweedie_deviance(self.y_true, self.y_pred)\n",
    "\n",
    "        return {\n",
    "            \"MAE\": mae,\n",
    "            \"MSE\": mse,\n",
    "            \"RMSE\": rmse,\n",
    "            \"MAPE\": mape,\n",
    "            \"R2\": r2,\n",
    "            \"Accuracy\": accuracy,\n",
    "            \"Explained Variance\": explained_variance,\n",
    "            \"Max Error\": max_err,\n",
    "            \"Mean Poisson Deviance\": poisson_deviance,\n",
    "            \"Mean Gamma Deviance\": gamma_deviance,\n",
    "            \"Mean Tweedie Deviance\": tweedie_deviance\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c35241bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation metrics saved to lstm_evaluation.txt\n"
     ]
    }
   ],
   "source": [
    "# Usage\n",
    "result = Result(lstm_model, test_data_gen)\n",
    "evaluation = result.evaluate()\n",
    "clear_output()\n",
    "y_true_lstm = result.y_true\n",
    "y_pred_lstm = result.y_pred\n",
    "# Save the output to a text file\n",
    "output_filename = \"lstm_evaluation.txt\"\n",
    "with open(output_filename, \"w\") as output_file:\n",
    "    output_file.write(\"LSTM Model Evaluation Metrics --\\n\")\n",
    "    for metric, value in evaluation.items():\n",
    "        output_file.write(f\"{metric}: {value}\\n\")\n",
    "\n",
    "print(f\"Evaluation metrics saved to {output_filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f90cae5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {'y_true': y_true_lstm, 'y_pred': y_pred_lstm}\n",
    "\n",
    "# Create a DataFrame from the dictionary\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Specify the filename for the CSV file\n",
    "csv_filename = 'lstm_predictions.csv'\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "df.to_csv(csv_filename, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b87c9782",
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_model.save('Model.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91e059e4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a57ac22f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
